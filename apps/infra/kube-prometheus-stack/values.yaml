nameOverride: ""
namespaceOverride: ""
kubeTargetVersionOverride: ""
kubeVersionOverride: ""
commonLabels: {}
crds:
  enabled: true
customRules: {}

defaultRules:
  labels:
    prometheus.io/scrap-with: kube-prometheus-stack
  create: true
  rules:
    alertmanager: true
    etcd: true
    configReloaders: true
    general: true
    k8sContainerCpuUsageSecondsTotal: true
    k8sContainerMemoryCache: true
    k8sContainerMemoryRss: true
    k8sContainerMemorySwap: true
    k8sContainerResource: true
    k8sContainerMemoryWorkingSetBytes: true
    k8sPodOwner: true
    kubeApiserverAvailability: false
    kubeApiserverBurnrate: false
    kubeApiserverHistogram: false
    kubeApiserverSlos: false
    kubeControllerManager: false
    kubelet: true
    kubeProxy: true
    kubePrometheusGeneral: true
    kubePrometheusNodeRecording: true
    kubernetesApps: true
    kubernetesResources: true
    kubernetesStorage: true
    kubernetesSystem: true
    kubeSchedulerAlerting: true
    kubeSchedulerRecording: false
    kubeStateMetrics: true
    network: true
    node: true
    nodeExporterAlerting: true
    nodeExporterRecording: true
    prometheus: true
    prometheusOperator: true
    windows: false

  appNamespacesTarget: ".*"
  keepFiringFor: ""
  annotations: {}
  additionalRuleLabels: {}
  additionalRuleAnnotations: {}
  additionalRuleGroupLabels:
    alertmanager: {}
    etcd: {}
    configReloaders: {}
    general: {}
    k8sContainerCpuUsageSecondsTotal: {}
    k8sContainerMemoryCache: {}
    k8sContainerMemoryRss: {}
    k8sContainerMemorySwap: {}
    k8sContainerResource: {}
    k8sPodOwner: {}
    kubeApiserverAvailability: {}
    kubeApiserverBurnrate: {}
    kubeApiserverHistogram: {}
    kubeApiserverSlos: {}
    kubeControllerManager: {}
    kubelet: {}
    kubeProxy: {}
    kubePrometheusGeneral: {}
    kubePrometheusNodeRecording: {}
    kubernetesApps: {}
    kubernetesResources: {}
    kubernetesStorage: {}
    kubernetesSystem: {}
    kubeSchedulerAlerting: {}
    kubeSchedulerRecording: {}
    kubeStateMetrics: {}
    network: {}
    node: {}
    nodeExporterAlerting: {}
    nodeExporterRecording: {}
    prometheus: {}
    prometheusOperator: {}

  ## Additional annotations for specific PrometheusRule alerts groups
  additionalRuleGroupAnnotations:
    alertmanager: {}
    etcd: {}
    configReloaders: {}
    general: {}
    k8sContainerCpuUsageSecondsTotal: {}
    k8sContainerMemoryCache: {}
    k8sContainerMemoryRss: {}
    k8sContainerMemorySwap: {}
    k8sContainerResource: {}
    k8sPodOwner: {}
    kubeApiserverAvailability: {}
    kubeApiserverBurnrate: {}
    kubeApiserverHistogram: {}
    kubeApiserverSlos: {}
    kubeControllerManager: {}
    kubelet: {}
    kubeProxy: {}
    kubePrometheusGeneral: {}
    kubePrometheusNodeRecording: {}
    kubernetesApps: {}
    kubernetesResources: {}
    kubernetesStorage: {}
    kubernetesSystem: {}
    kubeSchedulerAlerting: {}
    kubeSchedulerRecording: {}
    kubeStateMetrics: {}
    network: {}
    node: {}
    nodeExporterAlerting: {}
    nodeExporterRecording: {}
    prometheus: {}
    prometheusOperator: {}

  additionalAggregationLabels: []

  ## Prefix for runbook URLs. Use this to override the first part of the runbookURLs that is common to all rules.
  runbookUrl: "https://runbooks.prometheus-operator.dev/runbooks"
  ## Disabled PrometheusRule alerts
  disabled:
    etcdHighNumberOfFailedGRPCRequests: true

additionalPrometheusRulesMap: {}

global:
  rbac:
    create: true
    createAggregateClusterRoles: false
    pspEnabled: false
    pspAnnotations: {}
  imageRegistry: ""
  imagePullSecrets: []

windowsMonitoring:
  ## Deploys the windows-exporter and Windows-specific dashboards and rules (job name must be 'windows-exporter')
  enabled: false

prometheus-windows-exporter:
  prometheus:
    monitor:
      enabled: false
      jobLabel: jobLabel
  releaseLabel: true
  podLabels:
    jobLabel: windows-exporter
  config: |-
    collectors:
      enabled: '[defaults],memory,container'

####################################################

alertmanager:
  # fullnameOverride: "alertmanager"
  enabled: true
  annotations: {}
  apiVersion: v2
  enableFeatures: []
  serviceAccount:
    create: true
    name: ""
    annotations: {}
    automountServiceAccountToken: true
  podDisruptionBudget:
    enabled: false
    minAvailable: 1
    maxUnavailable: ""
  config:
    global:
      resolve_timeout: 5m
    inhibit_rules:
    - source_matchers:
      - 'severity = critical'
      target_matchers:
      - 'severity =~ warning|info'
      equal:
      - 'namespace'
      - 'alertname'
    - source_matchers:
      - 'severity = warning'
      target_matchers:
      - 'severity = info'
      equal:
      - 'namespace'
      - 'alertname'
    - source_matchers:
      - 'alertname = InfoInhibitor'
      target_matchers:
      - 'severity = info'
      equal:
      - 'namespace'
    - target_matchers:
      - 'alertname = InfoInhibitor'
    route:
      group_by: ['namespace']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 12h
      receiver: 'null'
      routes:
      - receiver: 'null'
        matchers:
        - alertname = "Watchdog"
    receivers:
    - name: 'null'
    templates:
    - '/etc/alertmanager/config/*.tmpl'

  stringConfig: ""
  tplConfig: false
  templateFiles: {}

  ingress:
    enabled: false

  ## Configuration for Alertmanager service
  service:
    annotations: {}
    labels: {}
    clusterIP: ""
    port: 9093
    targetPort: 9093
    nodePort: 30903
    additionalPorts: []
    externalIPs: []
    loadBalancerIP: "192.168.40.195"
    loadBalancerSourceRanges: []
    externalTrafficPolicy: Cluster
    type: LoadBalancer
    sessionAffinity: None
    sessionAffinityConfig:
      clientIP:
        timeoutSeconds: 10800

  servicePerReplica:
    enabled: false
    annotations: {}
    port: 9093
    targetPort: 9093
    nodePort: 30904
    loadBalancerSourceRanges: []
    externalTrafficPolicy: Cluster
    type: ClusterIP

  serviceMonitor:
    interval: "15s"
    selfMonitor: true
    additionalLabels:
      prometheus.io/scrap-with: kube-prometheus-stack
    sampleLimit: 0
    targetLimit: 0
    labelLimit: 0
    labelNameLengthLimit: 0
    labelValueLengthLimit: 0
    proxyUrl: ""
    scheme: ""
    enableHttp2: false
    tlsConfig: {}
    bearerTokenFile:
    metricRelabelings: []
    relabelings: []

  ## Settings affecting alertmanagerSpec
  alertmanagerSpec:
    podMetadata: {}
    useExistingSecret: false
    secrets: []
    automountServiceAccountToken: true
    configMaps: []
    web: {}
    alertmanagerConfigSelector:
      matchLabels:
        prometheus.io/scrap-with: kube-prometheus-stack
    alertmanagerConfigNamespaceSelector: {}
    alertmanagerConfiguration: {}
    alertmanagerConfigMatcherStrategy: {}
    logFormat: logfmt
    logLevel: info
    replicas: 1
    retention: 120h

    storage:
      volumeClaimTemplate:
        metadata:
          annotations:
            argocd.argoproj.io/sync-wave: "-1"
            velero.io/csi-volumesnapshot-class: "longhorn-backup-vsc"
          labels:
            backup: pvc-kube-stack
          name: pvc-kube-alertmanager
        spec:
          accessModes:
          - ReadWriteOnce
          storageClassName: longhorn
          resources:
            requests:
              storage: 4Gi
          volumeName: pv-kube-alertmanager # must reference Persistent Volume

    externalUrl:
    routePrefix: /
    scheme: ""
    tlsConfig: {}
    paused: false
    nodeSelector: {}
    resources:
      requests:
        cpu: 250m
        memory: 256Mi
      limits:
        cpu: 350m
        memory: 256Mi
    podAntiAffinity: ""
    podAntiAffinityTopologyKey: kubernetes.io/hostname
    affinity: {}
    tolerations: []
    topologySpreadConstraints: []
    securityContext:
      runAsGroup: 2000
      runAsNonRoot: true
      runAsUser: 1000
      fsGroup: 2000
      seccompProfile:
        type: RuntimeDefault
    listenLocal: false
    containers: []
    volumes: []
    volumeMounts: []
    initContainers: []
    priorityClassName: ""
    additionalPeers: []
    portName: "http-web"
    clusterAdvertiseAddress: false
    clusterGossipInterval: ""
    clusterPeerTimeout: ""
    clusterPushpullInterval: ""
    forceEnableClusterMode: false
    minReadySeconds: 0
    additionalConfig: {}
    additionalConfigString: ""
  extraSecret:
    annotations: {}
    data: {}

####################################################

grafana:
  enabled: true
  namespaceOverride: ""
  # fullnameOverride: "grafana"
  forceDeployDatasources: true
  forceDeployDashboards: false
  defaultDashboardsEnabled: true
  defaultDashboardsTimezone: cet
  defaultDashboardsEditable: true
  admin:
    existingSecret: grafana-admin-secret
    userKey: username
    passwordKey: password
  rbac:
    pspEnabled: false
  ingress:
    enabled: false
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: preferred
            operator: In
            values:
            - observability

  persistence:
    enabled: true
    existingClaim: pvc-kube-grafana

  serviceAccount:
    create: true
    autoMount: true

##############################

  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:

      - name: "devices"
        allowUiUpdates: true
        disableDeletion: false
        editable: true
        folder: "Devices"
        options:
          path: /var/lib/grafana/dashboards/device
        orgId: 1
        type: file

      - name: "extra"
        allowUiUpdates: true
        disableDeletion: false
        editable: true
        folder: "Extra"
        options:
          path: /var/lib/grafana/dashboards/extra
        orgId: 1
        type: file

      - name: "k3s"
        allowUiUpdates: true
        disableDeletion: false
        editable: true
        folder: "k3s"
        orgId: 1
        options:
          path: /var/lib/grafana/dashboards/k3s
        type: file

      - name: "network"
        allowUiUpdates: true
        disableDeletion: false
        editable: true
        folder: "Network"
        orgId: 1
        options:
          path: /var/lib/grafana/dashboards/network
        type: file

      - name: "storage"
        allowUiUpdates: true
        disableDeletion: false
        editable: true
        folder: "Storage"
        orgId: 1
        options:
          path: /var/lib/grafana/dashboards/storage
        type: file

  ## Configure grafana dashboard
  dashboards:

    devices:
      prometheus-pve-exporter:
        datasource:
        - name: DS_PROMETHEUS
          value: Prometheus
        gnetId: 10347
        revision: 5

      blackbox-exporter-overview:
        datasource:
        - name: DS_PROMETHEUS
          value: Prometheus
        gnetId: 5345

    extra:
      argocd-overview:
        datasource:
        - name: DS_PROMETHEUS
          value: Prometheus
        gnetId: 14584

      freqtrade:
        datasource:
        - name: DS_PROMETHEUS
          value: Prometheus
        gnetId: 14632

    k3s:
      k3s-kubelet:
        datasource:
        - name: DS_PROMETHEUS
          value: Prometheus
        gnetId: 16361

      k3s-proxy:
        datasource:
        - name: DS_PROMETHEUS
          value: Prometheus
        gnetId: 12129

      k3s-controller-manager:
        datasource:
        - name: DS_PROMETHEUS
          value: Prometheus
        gnetId: 12122

      k3s-scheduler:
        datasource:
        - name: DS_PROMETHEUS
          value: Prometheus
        gnetId: 12130

      k3s-etcd:
        datasource:
        - name: DS_PROMETHEUS
          value: Prometheus
        gnetId: 16359

      k3s-api-server:
        datasources:
        - name: DS_PROMETHEUS
          value: Prometheus
        gnetId: 12654

    network:
      mikrotik:
        datasource:
        - name: DS_PROMETHEUS
          value: Prometheus
        gnetId: 13679
        revision: 22

      cert-manager:
        datasource:
        - name: DS_PROMETHEUS
          value: Prometheus
        gnetId: 20842
        revision: 2

      external-dns:
        datasource:
        - name: DS_PROMETHEUS
          value: Prometheus
        gnetId: 15038
        revision: 3

      authentik:
        datasource:
        - name: DS_PROMETHEUS
          value: Prometheus
        gnetId: 14837

      traefik:
        datasource:
        - name: DS_PROMETHEUS
          value: Prometheus
        gnetId: 18111
        revision: 1

    storage:
      minio-dashboard:
        datasources:
        - name: DS_PROMETHEUS
          value: Prometheus
        gnetId: 13502
        revision: 26

      minio-bucket:
        datasources:
        - name: DS_PROMETHEUS
          value: Prometheus
        gnetId: 19237

      longhorn:
        datasource:
        - name: DS_PROMETHEUS
          value: Prometheus
        gnetId: 16888

      velero:
        datasource:
        - name: DS_PROMETHEUS
          value: Prometheus
        gnetId: 15469

############# sidecar test ##########
  sidecar:
    alerts:
      enabled: true
      searchNamespace: ALL

    dashboards:
      enabled: true
      label: grafana_dashboard
      labelValue: "1"
      searchNamespace: ALL
      enableNewTablePanelSyntax: false
      annotations:
        # Puts kube-prometheus-stack default dashboards in 'Kubernetes' folder
        grafana_folder: /var/lib/grafana/dashboards/Kubernetes
      multicluster:
        global:
          enabled: false
        etcd:
          enabled: false
      provider:
        name: sidecarProvider
        orgid: "1"
        folder: ""
        type: file
        disableDelete: false
        allowUiUpdates: true
        foldersFromFilesStructure: true

#############################################

    datasources:
      enabled: true
      defaultDatasourceEnabled: true
      isDefaultDatasource: true
      name: Prometheus
      type: prometheus
      uid: prometheus
      annotations: {}
      httpMethod: POST
      createPrometheusReplicasDatasources: false
      label: grafana_datasource
      labelValue: "1"
      exemplarTraceIdDestinations:
        datasourceUid: tempo
        traceIdLabelName: trace_id
      alertmanager:
        enabled: true
        name: Alertmanager
        uid: alertmanager
        handleGrafanaManagedAlerts: false
        implementation: prometheus

  extraConfigmapMounts: []
  # - name: certs-configmap
  #   mountPath: /etc/grafana/ssl/
  #   configMap: certs-configmap
  #   readOnly: true

  deleteDatasources: []

  additionalDataSources:
  - name: Loki
    type: loki
    url: http://loki-gateway.monitoring
  # More config at https://grafana.com/docs/grafana/latest/datasources/tempo/configure-tempo-data-source/
  - name: Tempo
    uid: tempo
    type: tempo
    access: proxy
    url: http://tempo.monitoring.svc:3100
    jsonData:
      tracesToMetrics:
        datasourceUid: 'prom'
        spanStartTimeShift: '1h'
        spanEndTimeShift: '-1h'
        tags: [{ key: 'service.name', value: 'service' }]
        queries:
        - name: 'latency'
          query: 'sum(rate(traces_spanmetrics_latency_bucket{$$__tags}[5m]))'
      serviceMap:
        datasourceUid: 'prometheus'
      nodeGraph:
        enabled: true
      search:
        hide: false
      traceQuery:
        timeShiftEnabled: true
        spanStartTimeShift: '1h'
        spanEndTimeShift: '-1h'

  - name: prometheus-longhorn
    uid: longhorn
    type: prometheus
    access: proxy
    editable: true
    url: http://prometheus-longhorn.longhorn-system.svc:9090
    version: 1

  service:
    portName: http-web
    ipFamilies: []
    ipFamilyPolicy: ""

  serviceMonitor:
    enabled: true
    path: "/metrics"
    labels:
      prometheus.io/scrap-with: kube-prometheus-stack
    interval: ""
    scheme: http
    tlsConfig: {}
    scrapeTimeout: 30s
    relabelings:
    - sourceLabels:
      - __address__
      action: replace
      targetLabel: job
      replacement: grafana

####################################################
kubernetesServiceMonitors:
  enabled: true

kubeApiServer:
  enabled: true # false
  tlsConfig:
    serverName: kubernetes
    insecureSkipVerify: true # false
  serviceMonitor:
    interval: ""
    sampleLimit: 0
    targetLimit: 0
    labelLimit: 0
    labelNameLengthLimit: 0
    labelValueLengthLimit: 0
    proxyUrl: ""
    jobLabel: component
    selector:
      matchLabels:
        component: apiserver
        provider: kubernetes
    metricRelabelings:
    # Drop excessively noisy apiserver buckets.
    - action: drop
      regex: apiserver_request_duration_seconds_bucket;(0.15|0.2|0.3|0.35|0.4|0.45|0.6|0.7|0.8|0.9|1.25|1.5|1.75|2|3|3.5|4|4.5|6|7|8|9|15|25|40|50)
      sourceLabels:
      - __name__
      - le
    relabelings: []
    additionalLabels:
      prometheus.io/scrap-with: kube-prometheus-stack

## Component scraping the kubelet and kubelet-hosted cAdvisor
kubelet:
  enabled: true
  namespace: kube-system
  serviceMonitor:
    enabled: true
    additionalLabels:
      prometheus.io/scrap-with: kube-prometheus-stack
    attachMetadata:
      node: true # false
    interval: ""
    honorLabels: true
    honorTimestamps: true
    sampleLimit: 0
    targetLimit: 0
    labelLimit: 0
    labelNameLengthLimit: 0
    labelValueLengthLimit: 0
    proxyUrl: ""
    https: true
    insecureSkipVerify: true
    cAdvisor: true
    probes: true
    resource: false
    resourcePath: "/metrics/resource/v1alpha1"
    cAdvisorMetricRelabelings:
    # Drop less useful container CPU metrics.
    - sourceLabels: [__name__]
      action: drop
      regex: 'container_cpu_(cfs_throttled_seconds_total|load_average_10s|system_seconds_total|user_seconds_total)'
    # Drop less useful container / always zero filesystem metrics.
    - sourceLabels: [__name__]
      action: drop
      regex: 'container_fs_(io_current|io_time_seconds_total|io_time_weighted_seconds_total|reads_merged_total|sector_reads_total|sector_writes_total|writes_merged_total)'
    # Drop less useful / always zero container memory metrics.
    - sourceLabels: [__name__]
      action: drop
      regex: 'container_memory_(mapped_file|swap)'
    # Drop less useful container process metrics.
    - sourceLabels: [__name__]
      action: drop
      regex: 'container_(file_descriptors|tasks_state|threads_max)'
    # Drop container spec metrics that overlap with kube-state-metrics.
    - sourceLabels: [__name__]
      action: drop
      regex: 'container_spec.*'
    # Drop cgroup metrics with no pod.
    - sourceLabels: [id, pod]
      action: drop
      regex: '.+;'
    probesMetricRelabelings: []

    cAdvisorRelabelings:
    - action: replace
      sourceLabels: [__metrics_path__]
      targetLabel: metrics_path
    probesRelabelings:
    - action: replace
      sourceLabels: [__metrics_path__]
      targetLabel: metrics_path
    resourceRelabelings:
    - action: replace
      sourceLabels: [__metrics_path__]
      targetLabel: metrics_path
    metricRelabelings: []
    relabelings:
    - action: replace
      sourceLabels: [__metrics_path__]
      targetLabel: metrics_path

kubeControllerManager:
  enabled: true # false
  endpoints: # ips of servers
  # - 192.168.40.110
  - 192.168.40.113
  - 192.168.40.114
  service:
    enabled: true
    port: 10252 # null
    targetPort: 10252 # null
  serviceMonitor:
    enabled: true
    interval: ""
    sampleLimit: 0
    targetLimit: 0
    labelLimit: 0
    labelNameLengthLimit: 0
    labelValueLengthLimit: 0
    proxyUrl: ""
    port: http-metrics
    jobLabel: jobLabel
    selector: {}
    https: false
    insecureSkipVerify: null
    serverName: null
    metricRelabelings: []
    relabelings: []
    additionalLabels:
      prometheus.io/scrap-with: kube-prometheus-stack

## Component scraping coreDns. Use either this or kubeDns
coreDns:
  enabled: true
  service:
    enabled: true
    port: 9153
    targetPort: 9153

  serviceMonitor:
    enabled: true
    interval: ""
    sampleLimit: 0
    targetLimit: 0
    labelLimit: 0
    labelNameLengthLimit: 0
    labelValueLengthLimit: 0
    proxyUrl: ""
    port: http-metrics
    jobLabel: jobLabel
    selector: {}
    metricRelabelings: []
    relabelings: []
    additionalLabels:
      prometheus.io/scrap-with: kube-prometheus-stack

## Component scraping kubeDns. Use either this or coreDns
kubeDns:
  enabled: true # false
  service:
    dnsmasq:
      port: 10054
      targetPort: 10054
    skydns:
      port: 10055
      targetPort: 10055
    ipDualStack:
      enabled: false
      ipFamilies: ["IPv6", "IPv4"]
      ipFamilyPolicy: "PreferDualStack"
    # selector:
    #   k8s-app: kube-dns
  serviceMonitor:
    interval: ""
    sampleLimit: 0
    targetLimit: 0
    labelLimit: 0
    labelNameLengthLimit: 0
    labelValueLengthLimit: 0
    proxyUrl: ""

    jobLabel: jobLabel
    selector: {}
    metricRelabelings: []
    relabelings: []
    dnsmasqMetricRelabelings: []
    dnsmasqRelabelings: []
    additionalLabels:
      prometheus.io/scrap-with: kube-prometheus-stack

## Component scraping etcd
kubeEtcd:
  enabled: true # false
  endpoints: # ips of servers
  # - 192.168.40.110
  - 192.168.40.113
  - 192.168.40.114
  service:
    enabled: true
    port: 2381
    targetPort: 2381
    ipDualStack:
      enabled: false
      ipFamilies: ["IPv6", "IPv4"]
      ipFamilyPolicy: "PreferDualStack"
    # selector:
    #   component: etcd
  serviceMonitor:
    enabled: true
    interval: ""
    sampleLimit: 0
    targetLimit: 0
    labelLimit: 0
    labelNameLengthLimit: 0
    labelValueLengthLimit: 0
    proxyUrl: ""
    scheme: http
    insecureSkipVerify: false
    serverName: ""
    caFile: ""
    certFile: ""
    keyFile: ""
    port: http-metrics
    jobLabel: jobLabel
    selector: {}
    metricRelabelings: []
    relabelings: []
    additionalLabels:
      prometheus.io/scrap-with: kube-prometheus-stack

## Component scraping kube scheduler
kubeScheduler:
  enabled: true # false
  endpoints: # ips of servers
  # - 192.168.40.110
  - 192.168.40.113
  - 192.168.40.114
  service:
    enabled: true
    port: 10251 # null
    targetPort: 10251 # null
  serviceMonitor:
    enabled: true
    interval: ""
    sampleLimit: 0
    targetLimit: 0
    labelLimit: 0
    labelNameLengthLimit: 0
    labelValueLengthLimit: 0
    proxyUrl: ""
    https: false
    port: http-metrics
    jobLabel: jobLabel
    selector: {}
    #  matchLabels:
    #    component: kube-scheduler

    ## Skip TLS certificate validation when scraping
    insecureSkipVerify: null
    ## Name of the server to use when validating TLS certificate
    serverName: null

    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    metricRelabelings: []
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]

    ## RelabelConfigs to apply to samples before scraping
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    relabelings: []
    additionalLabels:
      prometheus.io/scrap-with: kube-prometheus-stack

## Component scraping kube proxy
kubeProxy:
  enabled: true # false
  endpoints: # ips of servers
  # - 192.168.40.110
  - 192.168.40.113
  - 192.168.40.114
  service:
    enabled: true
    port: 10249
    targetPort: 10249
  serviceMonitor:
    enabled: true
    interval: ""
    sampleLimit: 0
    targetLimit: 0
    labelLimit: 0
    labelNameLengthLimit: 0
    labelValueLengthLimit: 0
    proxyUrl: ""
    port: http-metrics
    jobLabel: jobLabel
    selector: {}
    https: false
    metricRelabelings: []
    relabelings: []
    additionalLabels:
      prometheus.io/scrap-with: kube-prometheus-stack

## Component scraping kube state metrics
kubeStateMetrics:
  enabled: true
kube-state-metrics:
  fullnameOverride: kube-state-metrics
  namespaceOverride: ""
  rbac:
    create: true
  releaseLabel: true
  prometheus:
    monitor:
      enabled: true
      additionalLabels:
        prometheus.io/scrap-with: kube-prometheus-stack
      interval: ""
      sampleLimit: 0
      targetLimit: 0
      labelLimit: 0
      labelNameLengthLimit: 0
      labelValueLengthLimit: 0
      scrapeTimeout: ""
      proxyUrl: ""
      honorLabels: true
      metricRelabelings: []
      relabelings: []

  selfMonitor:
    enabled: true # false

## Deploy node exporter as a daemonset to all nodes
nodeExporter:
  enabled: true
  operatingSystems:
    linux:
      enabled: true
    darwin:
      enabled: false

  ## ForceDeployDashboard Create dashboard configmap even if nodeExporter deployment has been disabled
  forceDeployDashboards: false

## Configuration for prometheus-node-exporter subchart
prometheus-node-exporter:
  fullnameOverride: prometheus-node-exporter
  namespaceOverride: ""
  podLabels:
    ## Add the 'node-exporter' label to be used by serviceMonitor to match standard common usage in rules and grafana dashboards
    jobLabel: node-exporter
  releaseLabel: true
  extraArgs:
  - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
  - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
  service:
    annotations:
      prometheus.io/scrape: "true"
    type: ClusterIP
    clusterIP: ""
    port: 9101
    servicePort: ""
    targetPort: 9101
    portName: metrics # http-metrics
    nodePort: null
    ipDualStack:
      enabled: false
      ipFamilies: ["IPv6", "IPv4"]
      ipFamilyPolicy: "PreferDualStack"
  prometheus:
    monitor:
      enabled: true
      additionalLabels:
        prometheus.io/scrap-with: kube-prometheus-stack
      jobLabel: jobLabel
      interval: "15s"
      sampleLimit: 0
      targetLimit: 0
      labelLimit: 0
      labelNameLengthLimit: 0
      labelValueLengthLimit: 0
      scrapeTimeout: ""
      proxyUrl: ""
      metricRelabelings: # []
      - sourceLabels: [__name__]
        separator: ;
        regex: ^node_mountstats_nfs_(event|operations|transport)_.+
        replacement: $1
        action: drop

      ## RelabelConfigs to apply to samples before scraping
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      relabelings: # []
      - sourceLabels: [__meta_kubernetes_pod_node_name]
        separator: ;
        regex: ^(.*)$
        targetLabel: nodename
        replacement: $1
        action: replace
  rbac:
    pspEnabled: false

## Manages Prometheus and Alertmanager components
prometheusOperator:
  enabled: true
  fullnameOverride: ""
  revisionHistoryLimit: 5
  strategy: {}
  tls:
    enabled: true
    tlsMinVersion: VersionTLS13
    # The default webhook port is 10250 in order to work out-of-the-box in GKE private clusters and avoid adding firewall rules.
    internalPort: 10250

  ## Admission webhook support for PrometheusRules resources added in Prometheus Operator 0.30 can be enabled to prevent incorrectly formatted
  ## rules from making their way into prometheus and potentially preventing the container from starting
  admissionWebhooks:
    ## Valid values: Fail, Ignore, IgnoreOnInstallOnly
    ## IgnoreOnInstallOnly - If Release.IsInstall returns "true", set "Ignore" otherwise "Fail"
    failurePolicy: ""
    ## The default timeoutSeconds is 10 and the maximum value is 30.
    timeoutSeconds: 10
    enabled: true
    ## A PEM encoded CA bundle which will be used to validate the webhook's server certificate.
    ## If unspecified, system trust roots on the apiserver are used.
    caBundle: ""
    ## If enabled, generate a self-signed certificate, then patch the webhook configurations with the generated data.
    ## On chart upgrades (or if the secret exists) the cert will not be re-generated. You can use this to provide your own
    ## certs ahead of time if you wish.
    annotations: {}
    #   argocd.argoproj.io/hook: PreSync
    #   argocd.argoproj.io/hook-delete-policy: HookSucceeded

    namespaceSelector: {}
    objectSelector: {}

    mutatingWebhookConfiguration:
      annotations: {}
      #   argocd.argoproj.io/hook: PreSync

    validatingWebhookConfiguration:
      annotations: {}
      #   argocd.argoproj.io/hook: PreSync

    deployment:
      enabled: false
      replicas: 1
      strategy: {}

      # Ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/
      podDisruptionBudget: {}
        # maxUnavailable: 1
        # minAvailable: 1

      ## Number of old replicasets to retain ##
      ## The default value is 10, 0 will garbage-collect old replicasets ##
      revisionHistoryLimit: 5
      tls:
        enabled: true
        # Value must match version names from https://golang.org/pkg/crypto/tls/#pkg-constants
        tlsMinVersion: VersionTLS13
        # The default webhook port is 10250 in order to work out-of-the-box in GKE private clusters and avoid adding firewall rules.
        internalPort: 10250

      ## Service account for Prometheus Operator Webhook to use.
      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
      serviceAccount:
        automountServiceAccountToken: false
        create: true
        name: "prometheus"

      ## Configuration for Prometheus operator Webhook service
      service:
        annotations: {}
        labels: {}
        clusterIP: ""
        ipDualStack:
          enabled: false
          ipFamilies: ["IPv6", "IPv4"]
          ipFamilyPolicy: "PreferDualStack"

        ## Port to expose on each node
        ## Only used if service.type is 'NodePort'
        nodePort: 31080
        nodePortTls: 31443

        ## Additional ports to open for Prometheus operator Webhook service
        ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#multi-port-services
        additionalPorts: []
        ## Loadbalancer IP
        ## Only use if service.type is "LoadBalancer"
        loadBalancerIP: ""
        loadBalancerSourceRanges: []
        ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
        externalTrafficPolicy: Cluster
        ## Service type
        ## NodePort, ClusterIP, LoadBalancer
        type: ClusterIP

        ## List of IP addresses at which the Prometheus server service is available
        ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
        externalIPs: []

      # ## Labels to add to the operator webhook deployment
      labels: {}
      annotations: {}
      podLabels: {}
      podAnnotations: {}

      ## Assign a PriorityClassName to pods if set
      # priorityClassName: ""

      ## Define Log Format
      # Use logfmt (default) or json logging
      logFormat: json # logfmt
      ## Decrease log verbosity to errors only
      logLevel: error

      ## Prometheus-operator webhook image
      ##
      image:
        registry: quay.io
        repository: prometheus-operator/admission-webhook
        # if not set appVersion field from Chart.yaml is used
        tag: ""
        sha: ""
        pullPolicy: IfNotPresent

      ## Define Log Format
      # Use logfmt (default) or json logging
        logFormat: json # logfmt

      ## Decrease log verbosity to errors only
      # logLevel: error

      ## Liveness probe
      livenessProbe:
        enabled: true
        failureThreshold: 3
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1

      ## Readiness probe
      readinessProbe:
        enabled: true
        failureThreshold: 3
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1

      ## Resource limits & requests
      resources:
        limits:
          cpu: 200m
          memory: 200Mi
        requests:
          cpu: 100m
          memory: 100Mi

      # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico),
      # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working
      hostNetwork: false
      nodeSelector: {}
      tolerations: []
      # - key: "key"
      #   operator: "Equal"
      #   value: "value"
      #   effect: "NoSchedule"

      ## Assign custom affinity rules to the prometheus operator
      ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
      affinity: {}
        # nodeAffinity:
        #   requiredDuringSchedulingIgnoredDuringExecution:
        #     nodeSelectorTerms:
        #     - matchExpressions:
        #       - key: kubernetes.io/e2e-az-name
        #         operator: In
        #         values:
        #         - e2e-az1
      #         - e2e-az2
      dnsConfig: {}
        # nameservers:
        #   - 1.2.3.4
        # searches:
        #   - ns1.svc.cluster-domain.example
        #   - my.dns.search.suffix
        # options:
        #   - name: ndots
        #     value: "2"
        #   - name: edns0
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
        seccompProfile:
          type: RuntimeDefault

      ## Container-specific security context configuration
      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
      ##
      containerSecurityContext:
        allowPrivilegeEscalation: false
        readOnlyRootFilesystem: true
        capabilities:
          drop:
          - ALL

      ## If false then the user will opt out of automounting API credentials.
      automountServiceAccountToken: true

    patch:
      enabled: true
      image:
        registry: registry.k8s.io
        repository: ingress-nginx/kube-webhook-certgen
        tag: v20221220-controller-v1.5.1-58-g787ea74b6
        sha: ""
        pullPolicy: IfNotPresent
      resources:
        requests:
          cpu: 250m
          memory: 256Mi
        limits:
          cpu: 550m
          memory: 256Mi

      ## Provide a priority class name to the webhook patching job
      priorityClassName: ""
      ttlSecondsAfterFinished: 60
      annotations: {}
      #   argocd.argoproj.io/hook: PreSync
      #   argocd.argoproj.io/hook-delete-policy: HookSucceeded
      podAnnotations: {}
      nodeSelector: {}
      affinity: {}
      tolerations: []

      ## SecurityContext holds pod-level security attributes and common container settings.
      ## This defaults to non root user with uid 2000 and gid 2000. *v1.PodSecurityContext  false
      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
      securityContext:
        runAsGroup: 2000
        runAsNonRoot: true
        runAsUser: 2000
        seccompProfile:
          type: RuntimeDefault
      ## Service account for Prometheus Operator Webhook Job Patch to use.
      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
      serviceAccount:
        create: true
        automountServiceAccountToken: true

    # Security context for create job container
    createSecretJob:
      securityContext:
        allowPrivilegeEscalation: false
        readOnlyRootFilesystem: true
        capabilities:
          drop:
          - ALL

      # Security context for patch job container
    patchWebhookJob:
      securityContext:
        allowPrivilegeEscalation: false
        readOnlyRootFilesystem: true
        capabilities:
          drop:
          - ALL

    # Use certmanager to generate webhook certs
    certManager:
      enabled: false
      # self-signed root certificate
      rootCert:
        duration: ""  # default to be 5y
      admissionCert:
        duration: ""  # default to be 1y
      # issuerRef:
      #   name: "issuer"
      #   kind: "ClusterIssuer"

  ## Namespaces to scope the interaction of the Prometheus Operator and the apiserver (allow list).
  ## This is mutually exclusive with denyNamespaces. Setting this to an empty object will disable the configuration
  ##
  namespaces: {}
    # releaseNamespace: true
    # additional:
    # - kube-system

  ## Namespaces not to scope the interaction of the Prometheus Operator (deny list).
  ##
  denyNamespaces: []

  ## Filter namespaces to look for prometheus-operator custom resources
  ##
  alertmanagerInstanceNamespaces: []
  alertmanagerConfigNamespaces: []
  prometheusInstanceNamespaces: []
  thanosRulerInstanceNamespaces: []

  ## The clusterDomain value will be added to the cluster.peer option of the alertmanager.
  ## Without this specified option cluster.peer will have value alertmanager-monitoring-alertmanager-0.alertmanager-operated:9094 (default value)
  ## With this specified option cluster.peer will have value alertmanager-monitoring-alertmanager-0.alertmanager-operated.namespace.svc.cluster-domain:9094
  ##
  # clusterDomain: "cluster.local"

  networkPolicy:
    ## Enable creation of NetworkPolicy resources.
    ##
    enabled: false

    ## Flavor of the network policy to use.
    #  Can be:
    #  * kubernetes for networking.k8s.io/v1/NetworkPolicy
    #  * cilium     for cilium.io/v2/CiliumNetworkPolicy
    flavor: kubernetes

    # cilium:
    #   egress:

    ## match labels used in selector
    # matchLabels: {}

  ## Service account for Prometheus Operator to use.
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  ##
  serviceAccount:
    create: true
    name: ""
    automountServiceAccountToken: true

  ## Configuration for Prometheus operator service
  ##
  service:
    annotations: {}
    labels: {}
    clusterIP: ""
    ipDualStack:
      enabled: false
      ipFamilies: ["IPv6", "IPv4"]
      ipFamilyPolicy: "PreferDualStack"

    nodePort: 30080
    nodePortTls: 30443
    additionalPorts: []
    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    externalTrafficPolicy: Cluster
    type: ClusterIP
    externalIPs: []

  labels: {}
  annotations: {}
  podLabels: {}

  podAnnotations: {}
  kubeletService:
    enabled: false
    namespace: kube-system
    selector: ""
    ## Use '{{ template "kube-prometheus-stack.fullname" . }}-kubelet' by default
    name: ""

  serviceMonitor:
    selfMonitor: true
    interval: ""
    sampleLimit: 0
    targetLimit: 0
    labelLimit: 0
    labelNameLengthLimit: 0
    labelValueLengthLimit: 0
    scrapeTimeout: ""
    metricRelabelings: []
    relabelings:
    - sourceLabels:
      - __address__
      action: replace
      targetLabel: job
      replacement: prometheus-operator
    additionalLabels:
      prometheus.io/scrap-with: kube-prometheus-stack

  resources:
    requests:
      cpu: 250m
      memory: 256Mi
    limits:
      cpu: 550m
      memory: 256Mi
  env:
    GOGC: "30"
  hostNetwork: false
  nodeSelector: {}
  tolerations: []
  affinity: {}
  dnsConfig: {}
  securityContext:
    fsGroup: 65534
    runAsGroup: 65534
    runAsNonRoot: true
    runAsUser: 65534
    seccompProfile:
      type: RuntimeDefault
  containerSecurityContext:
    allowPrivilegeEscalation: false
    readOnlyRootFilesystem: true
    capabilities:
      drop:
      - ALL

  # Enable vertical pod autoscaler support for prometheus-operator
  verticalPodAutoscaler:
    enabled: false

    # Recommender responsible for generating recommendation for the object.
    # List should be empty (then the default recommender will generate the recommendation)
    # or contain exactly one recommender.
    # recommenders:
    # - name: custom-recommender-performance

    # List of resources that the vertical pod autoscaler can control. Defaults to cpu and memory
    controlledResources: []
    # Specifies which resource values should be controlled: RequestsOnly or RequestsAndLimits.
    # controlledValues: RequestsAndLimits

    # Define the max allowed resources for the pod
    maxAllowed: {}
    # cpu: 200m
    # memory: 100Mi
    # Define the min allowed resources for the pod
    minAllowed: {}
    # cpu: 200m
    # memory: 100Mi

    updatePolicy:
      # Specifies minimal number of replicas which need to be alive for VPA Updater to attempt pod eviction
      # minReplicas: 1
      # Specifies whether recommended updates are applied when a Pod is started and whether recommended updates
      # are applied during the life of a Pod. Possible values are "Off", "Initial", "Recreate", and "Auto".
      updateMode: Auto

  ## Prometheus-operator image
  ##
  image:
    registry: quay.io
    repository: prometheus-operator/prometheus-operator
    # if not set appVersion field from Chart.yaml is used
    tag: ""
    sha: ""
    pullPolicy: IfNotPresent

  ## Prometheus image to use for prometheuses managed by the operator
  ##
  # prometheusDefaultBaseImage: prometheus/prometheus

  ## Prometheus image registry to use for prometheuses managed by the operator
  ##
  # prometheusDefaultBaseImageRegistry: quay.io

  ## Alertmanager image to use for alertmanagers managed by the operator
  ##
  # alertmanagerDefaultBaseImage: prometheus/alertmanager

  ## Alertmanager image registry to use for alertmanagers managed by the operator
  ##
  # alertmanagerDefaultBaseImageRegistry: quay.io

  ## Prometheus-config-reloader
  ##
  prometheusConfigReloader:
    image:
      registry: quay.io
      repository: prometheus-operator/prometheus-config-reloader
      # if not set appVersion field from Chart.yaml is used
      tag: ""
      sha: ""

    # add prometheus config reloader liveness and readiness probe. Default: false
    enableProbe: false

    # resource config for prometheusConfigReloader
    resources:
      requests:
        cpu: 200m
        memory: 50Mi
      limits:
        cpu: 200m
        memory: 50Mi

  ## Thanos side-car image when configured
  ##
  # thanosImage:
  #   registry: quay.io
  #   repository: thanos/thanos
  #   tag: v0.35.1
  #   sha: ""

  ## Set a Label Selector to filter watched prometheus and prometheusAgent
  prometheusInstanceSelector: ""

  ## Set a Label Selector to filter watched alertmanager
  alertmanagerInstanceSelector: ""

  ## Set a Label Selector to filter watched thanosRuler
  thanosRulerInstanceSelector: ""

  ## Set a Field Selector to filter watched secrets
  secretFieldSelector: "type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1"

  ## If false then the user will opt out of automounting API credentials.
  automountServiceAccountToken: true

  ## Additional volumes
  extraVolumes: []

  ## Additional volume mounts
  extraVolumeMounts: []

## Deploy a Prometheus instance
prometheus:
  enabled: true
  agentMode: false
  annotations: {}
  networkPolicy:
    enabled: false
    flavor: kubernetes

  serviceAccount:
    create: true
    name: ""
    annotations: {}
    automountServiceAccountToken: true

  # Service for thanos service discovery on sidecar
  # Enable this can make Thanos Query can use
  # `--store=dnssrv+_grpc._tcp.${kube-prometheus-stack.fullname}-thanos-discovery.${namespace}.svc.cluster.local` to discovery
  # Thanos sidecar on prometheus nodes
  # (Please remember to change ${kube-prometheus-stack.fullname} and ${namespace}. Not just copy and paste!)
  thanosService:
    enabled: false
    annotations: {}
    labels: {}

    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
    ##
    externalTrafficPolicy: Cluster

    ## Service type
    type: ClusterIP

    ## Service dual stack
    ipDualStack:
      enabled: false
      ipFamilies: ["IPv6", "IPv4"]
      ipFamilyPolicy: "PreferDualStack"

    ## gRPC port config
    portName: grpc
    port: 10901
    targetPort: "grpc"

    ## HTTP port config (for metrics)
    httpPortName: http
    httpPort: 10902
    targetHttpPort: "http"

    ## ClusterIP to assign
    # Default is to make this a headless service ("None")
    clusterIP: "None"

    ## Port to expose on each node, if service type is NodePort
    nodePort: 30901
    httpNodePort: 30902

  # ServiceMonitor to scrape Sidecar metrics
  # Needs thanosService to be enabled as well
  thanosServiceMonitor:
    enabled: false
    interval: ""
    additionalLabels: {}
    scheme: ""
    tlsConfig: {}
    bearerTokenFile:
    metricRelabelings: []
    relabelings: []

  # Service for external access to sidecar
  # Enabling this creates a service to expose thanos-sidecar outside the cluster.
  thanosServiceExternal:
    enabled: false
    annotations: {}
    labels: {}
    loadBalancerIP: ""
    loadBalancerSourceRanges: []

    ## gRPC port config
    portName: grpc
    port: 10901
    targetPort: "grpc"

    ## HTTP port config (for metrics)
    httpPortName: http
    httpPort: 10902
    targetHttpPort: "http"

    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
    externalTrafficPolicy: Cluster

    ## Service type
    type: LoadBalancer

    ## Port to expose on each node
    nodePort: 30901
    httpNodePort: 30902

  ## Configuration for Prometheus service
  service:
    annotations: {}
    labels: {}
    clusterIP: ""
    ipDualStack:
      enabled: false
      ipFamilies: ["IPv6", "IPv4"]
      ipFamilyPolicy: "PreferDualStack"
    port: 9090
    targetPort: 9090
    reloaderWebPort: 8080
    externalIPs: []
    nodePort: 30090
    loadBalancerIP: "192.168.40.194"
    loadBalancerSourceRanges: []
    externalTrafficPolicy: Cluster
    type: LoadBalancer # ClusterIP
    additionalPorts: []
    # additionalPorts:
    # - name: oauth-proxy
    #   port: 8081
    #   targetPort: 8081
    # - name: oauth-metrics
    #   port: 8082
    #   targetPort: 8082

    ## Consider that all endpoints are considered "ready" even if the Pods themselves are not
    ## Ref: https://kubernetes.io/docs/reference/kubernetes-api/service-resources/service-v1/#ServiceSpec
    publishNotReadyAddresses: false

    ## If you want to make sure that connections from a particular client are passed to the same Pod each time
    ## Accepts 'ClientIP' or 'None'
    sessionAffinity: None

    ## If you want to modify the ClientIP sessionAffinity timeout
    ## The value must be >0 && <=86400(for 1 day) if ServiceAffinity == "ClientIP"
    sessionAffinityConfig:
      clientIP:
        timeoutSeconds: 10800

  ## Configuration for creating a separate Service for each statefulset Prometheus replica
  servicePerReplica:
    enabled: false
    annotations: {}
    ## Port for Prometheus Service per replica to listen on
    port: 9090
    ## To be used with a proxy extraContainer port
    targetPort: 9090
    ## Port to expose on each node
    ## Only used if servicePerReplica.type is 'NodePort'
    nodePort: 30091
    ## Loadbalancer source IP ranges
    ## Only used if servicePerReplica.type is "LoadBalancer"
    loadBalancerSourceRanges: []
    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
    externalTrafficPolicy: Cluster
    ## Service type
    type: ClusterIP
    ## Service dual stack
    ipDualStack:
      enabled: false
      ipFamilies: ["IPv6", "IPv4"]
      ipFamilyPolicy: "PreferDualStack"

  ## Configure pod disruption budgets for Prometheus
  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget
  podDisruptionBudget:
    enabled: false
    minAvailable: 1
    maxUnavailable: ""

  # Ingress exposes thanos sidecar outside the cluster
  thanosIngress:
    enabled: false

  ## ExtraSecret can be used to store various data in an extra secret
  ## (use it for example to store hashed basic auth credentials)
  extraSecret:
    ## if not set, name will be auto generated
    # name: ""
    annotations: {}
    data: {}
  #   auth: |
  #     foo:$apr1$OFG3Xybp$ckL0FHDAkoXYIlH9.cysT0
  #     someoneelse:$apr1$DMZX2Z4q$6SbQIfyuLQd.xmo/P0m2c.

  ingress:
    enabled: false

    # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
    # ingressClassName: nginx

    annotations: {}
    labels: {}

    ## Redirect ingress to an additional defined port on the service
    # servicePort: 8081

    ## Hostnames.
    ## Must be provided if Ingress is enabled.
    ##
    # hosts:
    #   - prometheus.domain.com
    hosts: []

    ## Paths to use for ingress rules - one path should match the prometheusSpec.routePrefix
    ##
    paths: []
    # - /

    ## For Kubernetes >= 1.18 you should specify the pathType (determines how Ingress paths should be matched)
    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
    # pathType: ImplementationSpecific

    ## TLS configuration for Prometheus Ingress
    ## Secret must be manually created in the namespace
    ##
    tls: []
      # - secretName: prometheus-general-tls
      #   hosts:
      #     - prometheus.example.com

  ## Configuration for creating an Ingress that will map to each Prometheus replica service
  ## prometheus.servicePerReplica must be enabled
  ##
  ingressPerReplica:
    enabled: false

    # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
    # ingressClassName: nginx

    annotations: {}
    labels: {}

    ## Final form of the hostname for each per replica ingress is
    ## {{ ingressPerReplica.hostPrefix }}-{{ $replicaNumber }}.{{ ingressPerReplica.hostDomain }}
    ##
    ## Prefix for the per replica ingress that will have `-$replicaNumber`
    ## appended to the end
    hostPrefix: ""
    ## Domain that will be used for the per replica ingress
    hostDomain: ""

    ## Paths to use for ingress rules
    ##
    paths: []
    # - /

    ## For Kubernetes >= 1.18 you should specify the pathType (determines how Ingress paths should be matched)
    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
    # pathType: ImplementationSpecific

    ## Secret name containing the TLS certificate for Prometheus per replica ingress
    ## Secret must be manually created in the namespace
    tlsSecretName: ""

    ## Separated secret for each per replica Ingress. Can be used together with cert-manager
    ##
    tlsSecretPerReplica:
      enabled: false
      ## Final form of the secret for each per replica ingress is
      ## {{ tlsSecretPerReplica.prefix }}-{{ $replicaNumber }}
      prefix: "prometheus"

  ## Configure additional options for default pod security policy for Prometheus
  ## ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  podSecurityPolicy:
    allowedCapabilities: []
    allowedHostPaths: []
    volumes: []

  serviceMonitor:
    selfMonitor: true
    additionalLabels:
      prometheus.io/scrap-with: kube-prometheus-stack
    sampleLimit: 0
    targetLimit: 0
    labelLimit: 0
    labelNameLengthLimit: 0
    labelValueLengthLimit: 0
    scheme: ""
    tlsConfig: {}
    bearerTokenFile:
    metricRelabelings: []
    relabelings:
    - sourceLabels:
      - __address__
      action: replace
      targetLabel: job
      replacement: prometheus
    additionalEndpoints: []
  ## Settings affecting prometheusSpec
  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#prometheusspec
  ##
  prometheusSpec:
    persistentVolumeClaimRetentionPolicy: {}
    automountServiceAccountToken: true
    disableCompaction: false
    apiserverConfig: {}
    additionalArgs: []
    scrapeInterval: ""
    scrapeTimeout: ""
    scrapeClasses: []
    evaluationInterval: ""
    listenLocal: false
    enableAdminAPI: false
    version: ""
    web: {}
    exemplars: ""
    enableFeatures: []
    tolerations: []
    topologySpreadConstraints: []
    alertingEndpoints: []
    externalLabels: {}
    enableRemoteWriteReceiver: true
    replicaExternalLabelName: ""
    replicaExternalLabelNameClear: false
    prometheusExternalLabelName: ""
    prometheusExternalLabelNameClear: false
    externalUrl: ""
    nodeSelector: {}
    secrets: []
    configMaps: []
    query: {}
    ruleNamespaceSelector: {}

    ruleSelectorNilUsesHelmValues: false
    ruleSelector:
      matchLabels:
        prometheus.io/scrap-with: kube-prometheus-stack

    serviceMonitorSelectorNilUsesHelmValues: false
    serviceMonitorSelector:
      matchLabels:
        prometheus.io/scrap-with: kube-prometheus-stack

    serviceMonitorNamespaceSelector: {}
    podMonitorSelectorNilUsesHelmValues: false
    # podMonitorSelector: {}

    podMonitorNamespaceSelector: false
    podMonitorSelector:
      matchLabels:
        prometheus.io/scrap-with: kube-prometheus-stack

    probeSelectorNilUsesHelmValues: false
    probeSelector:
      matchLabels:
        prometheus.io/scrap-with: kube-prometheus-stack

    probeNamespaceSelector: {}
    scrapeConfigSelectorNilUsesHelmValues: false
    scrapeConfigSelector:
      matchLabels:
        prometheus.io/scrap-with: kube-prometheus-stack

    scrapeConfigNamespaceSelector: {}

    retention: 30d
    retentionSize: ""
    tsdb:
      outOfOrderTimeWindow: 0s
    walCompression: true
    paused: false
    replicas: 1
    shards: 1
    logLevel: info
    logFormat: logfmt
    routePrefix: /
    podMetadata: {}
    # labels:
    #   app: prometheus
    #   k8s-app: prometheus

    podAntiAffinity: ""
    podAntiAffinityTopologyKey: kubernetes.io/hostname
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 1
          preference:
            matchExpressions:
            - key: preferred
              operator: In
              values:
              - observability
    remoteRead: []
    additionalRemoteRead: []
    remoteWrite: []
    additionalRemoteWrite: []
    remoteWriteDashboards: false
    resources:
      requests:
        memory: 0.4Gi
      limits:
        memory: 1.1Gi

    storageSpec:
      volumeClaimTemplate:
        metadata:
          annotations:
            argocd.argoproj.io/sync-wave: "-1"
            velero.io/csi-volumesnapshot-class: "longhorn-backup-vsc"
          labels:
            backup: pvc-kube-stack
          name: pvc-kube-prometheus
        spec:
          accessModes:
          - ReadWriteOnce
          storageClassName: longhorn-retain
          resources:
            requests:
              storage: 40Gi
          volumeName: pv-kube-prometheus # must reference Persistent Volume

    volumes: []
    volumeMounts: []

    additionalScrapeConfigs: []
    # additionalScrapeConfigs: |

    # - job_name: kube-etcd
    #   kubernetes_sd_configs:
    #     - role: node
    #   scheme: https
    #   tls_config:
    #     ca_file:   /etc/prometheus/secrets/etcd-client-cert/etcd-ca
    #     cert_file: /etc/prometheus/secrets/etcd-client-cert/etcd-client
    #     key_file:  /etc/prometheus/secrets/etcd-client-cert/etcd-client-key
    #   relabel_configs:
    #   - action: labelmap
    #     regex: __meta_kubernetes_node_label_(.+)
    #   - source_labels: [__address__]
    #     action: replace
    #     targetLabel: __address__
    #     regex: ([^:;]+):(\d+)
    #     replacement: ${1}:2379
    #   - source_labels: [__meta_kubernetes_node_name]
    #     action: keep
    #     regex: .*mst.*
    #   - source_labels: [__meta_kubernetes_node_name]
    #     action: replace
    #     targetLabel: node
    #     regex: (.*)
    #     replacement: ${1}
    #   metric_relabel_configs:
    #   - regex: (kubernetes_io_hostname|failure_domain_beta_kubernetes_io_region|beta_kubernetes_io_os|beta_kubernetes_io_arch|beta_kubernetes_io_instance_type|failure_domain_beta_kubernetes_io_zone)
    #     action: labeldrop
    #
    ## If scrape config contains a repetitive section, you may want to use a template.
    ## In the following example, you can see how to define `gce_sd_configs` for multiple zones
    # additionalScrapeConfigs: |
    #  - job_name: "node-exporter"
    #    gce_sd_configs:
    #    {{range $zone := .Values.gcp_zones}}
    #    - project: "project1"
    #      zone: "{{$zone}}"
    #      port: 9100
    #    {{end}}
    #    relabel_configs:
    #    ...

    ## If additional scrape configurations are already deployed in a single secret file you can use this section.
    ## Expected values are the secret name and key
    ## Cannot be used with additionalScrapeConfigs
    additionalScrapeConfigsSecret: {}
      # enabled: false
      # name:
      # key:

    ## additionalPrometheusSecretsAnnotations allows to add annotations to the kubernetes secret. This can be useful
    ## when deploying via spinnaker to disable versioning on the secret, strategy.spinnaker.io/versioned: 'false'
    additionalPrometheusSecretsAnnotations: {}

    ## AdditionalAlertManagerConfigs allows for manual configuration of alertmanager jobs in the form as specified
    ## in the official Prometheus documentation https://prometheus.io/docs/prometheus/latest/configuration/configuration/#<alertmanager_config>.
    ## AlertManager configurations specified are appended to the configurations generated by the Prometheus Operator.
    ## As AlertManager configs are appended, the user is responsible to make sure it is valid. Note that using this
    ## feature may expose the possibility to break upgrades of Prometheus. It is advised to review Prometheus release
    ## notes to ensure that no incompatible AlertManager configs are going to break Prometheus after the upgrade.
    ##
    additionalAlertManagerConfigs: []
    # - consul_sd_configs:
    #   - server: consul.dev.test:8500
    #     scheme: http
    #     datacenter: dev
    #     tag_separator: ','
    #     services:
    #       - metrics-prometheus-alertmanager

    ## If additional alertmanager configurations are already deployed in a single secret, or you want to manage
    ## them separately from the helm deployment, you can use this section.
    ## Expected values are the secret name and key
    ## Cannot be used with additionalAlertManagerConfigs
    additionalAlertManagerConfigsSecret: {}
      # name:
      # key:
      # optional: false

    ## AdditionalAlertRelabelConfigs allows specifying Prometheus alert relabel configurations. Alert relabel configurations specified are appended
    ## to the configurations generated by the Prometheus Operator. Alert relabel configurations specified must have the form as specified in the
    ## official Prometheus documentation: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#alert_relabel_configs.
    ## As alert relabel configs are appended, the user is responsible to make sure it is valid. Note that using this feature may expose the
    ## possibility to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible alert relabel
    ## configs are going to break Prometheus after the upgrade.
    ##
    additionalAlertRelabelConfigs: []
    # - separator: ;
    #   regex: prometheus_replica
    #   replacement: $1
    #   action: labeldrop

    ## If additional alert relabel configurations are already deployed in a single secret, or you want to manage
    ## them separately from the helm deployment, you can use this section.
    ## Expected values are the secret name and key
    ## Cannot be used with additionalAlertRelabelConfigs
    additionalAlertRelabelConfigsSecret: {}
      # name:
      # key:

    ## SecurityContext holds pod-level security attributes and common container settings.
    ## This defaults to non root user with uid 1000 and gid 2000.
    ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md
    ##
    securityContext:
      runAsGroup: 2000
      runAsNonRoot: true
      runAsUser: 1000
      fsGroup: 2000
      seccompProfile:
        type: RuntimeDefault

    ## Priority class assigned to the Pods
    ##
    priorityClassName: ""

    ## Thanos configuration allows configuring various aspects of a Prometheus server in a Thanos environment.
    ## This section is experimental, it may change significantly without deprecation notice in any release.
    ## This is experimental and may change significantly without backward compatibility in any release.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#thanosspec
    ##
    thanos: {}
      # secretProviderClass:
      #   provider: gcp
      #   parameters:
      #     secrets: |
      #       - resourceName: "projects/$PROJECT_ID/secrets/testsecret/versions/latest"
      #         fileName: "objstore.yaml"
      ## ObjectStorageConfig configures object storage in Thanos.
      # objectStorageConfig:
      #   # use existing secret, if configured, objectStorageConfig.secret will not be used
      #   existingSecret: {}
      #     # name: ""
      #     # key: ""
      #   # will render objectStorageConfig secret data and configure it to be used by Thanos custom resource,
      #   # ignored when prometheusspec.thanos.objectStorageConfig.existingSecret is set
      #   # https://thanos.io/tip/thanos/storage.md/#s3
      #   secret: {}
      #     # type: S3
      #     # config:
      #     #   bucket: ""
      #     #   endpoint: ""
      #     #   region: ""
      #     #   access_key: ""
      #     #   secret_key: ""

    ## Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to a Prometheus pod.
    ## if using proxy extraContainer update targetPort with proxy container port
    containers: []
    # containers:
    # - name: oauth-proxy
    #   image: quay.io/oauth2-proxy/oauth2-proxy:v7.5.1
    #   args:
    #   - --upstream=http://127.0.0.1:9090
    #   - --http-address=0.0.0.0:8081
    #   - --metrics-address=0.0.0.0:8082
    #   - ...
    #   ports:
    #   - containerPort: 8081
    #     name: oauth-proxy
    #     protocol: TCP
    #   - containerPort: 8082
    #     name: oauth-metrics
    #     protocol: TCP
    #   resources: {}

    ## InitContainers allows injecting additional initContainers. This is meant to allow doing some changes
    ## (permissions, dir tree) on mounted volumes before starting prometheus
    initContainers: []

    ## PortName to use for Prometheus.
    ##
    portName: "http-web"

    ## ArbitraryFSAccessThroughSMs configures whether configuration based on a service monitor can access arbitrary files
    ## on the file system of the Prometheus container e.g. bearer token files.
    arbitraryFSAccessThroughSMs: false

    ## OverrideHonorLabels if set to true overrides all user configured honor_labels. If HonorLabels is set in ServiceMonitor
    ## or PodMonitor to true, this overrides honor_labels to false.
    overrideHonorLabels: false

    ## OverrideHonorTimestamps allows to globally enforce honoring timestamps in all scrape configs.
    overrideHonorTimestamps: false

    ## When ignoreNamespaceSelectors is set to true, namespaceSelector from all PodMonitor, ServiceMonitor and Probe objects will be ignored,
    ## they will only discover targets within the namespace of the PodMonitor, ServiceMonitor and Probe object,
    ## and servicemonitors will be installed in the default service namespace.
    ## Defaults to false.
    ignoreNamespaceSelectors: false

    ## EnforcedNamespaceLabel enforces adding a namespace label of origin for each alert and metric that is user created.
    ## The label value will always be the namespace of the object that is being created.
    ## Disabled by default
    enforcedNamespaceLabel: ""

    ## PrometheusRulesExcludedFromEnforce - list of prometheus rules to be excluded from enforcing of adding namespace labels.
    ## Works only if enforcedNamespaceLabel set to true. Make sure both ruleNamespace and ruleName are set for each pair
    ## Deprecated, use `excludedFromEnforcement` instead
    prometheusRulesExcludedFromEnforce: []

    ## ExcludedFromEnforcement - list of object references to PodMonitor, ServiceMonitor, Probe and PrometheusRule objects
    ## to be excluded from enforcing a namespace label of origin.
    ## Works only if enforcedNamespaceLabel set to true.
    ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#objectreference
    excludedFromEnforcement: []

    ## QueryLogFile specifies the file to which PromQL queries are logged. Note that this location must be writable,
    ## and can be persisted using an attached volume. Alternatively, the location can be set to a stdout location such
    ## as /dev/stdout to log querie information to the default Prometheus log stream. This is only available in versions
    ## of Prometheus >= 2.16.0. For more details, see the Prometheus docs (https://prometheus.io/docs/guides/query-log/)
    queryLogFile: false

    # Use to set global sample_limit for Prometheus. This act as default SampleLimit for ServiceMonitor or/and PodMonitor.
    # Set to 'false' to disable global sample_limit. or set to a number to override the default value.
    sampleLimit: false

    # EnforcedKeepDroppedTargetsLimit defines on the number of targets dropped by relabeling that will be kept in memory.
    # The value overrides any spec.keepDroppedTargets set by ServiceMonitor, PodMonitor, Probe objects unless spec.keepDroppedTargets
    # is greater than zero and less than spec.enforcedKeepDroppedTargets. 0 means no limit.
    enforcedKeepDroppedTargets: 0

    ## EnforcedSampleLimit defines global limit on number of scraped samples that will be accepted. This overrides any SampleLimit
    ## set per ServiceMonitor or/and PodMonitor. It is meant to be used by admins to enforce the SampleLimit to keep overall
    ## number of samples/series under the desired limit. Note that if SampleLimit is lower that value will be taken instead.
    enforcedSampleLimit: false

    ## EnforcedTargetLimit defines a global limit on the number of scraped targets. This overrides any TargetLimit set
    ## per ServiceMonitor or/and PodMonitor. It is meant to be used by admins to enforce the TargetLimit to keep the overall
    ## number of targets under the desired limit. Note that if TargetLimit is lower, that value will be taken instead, except
    ## if either value is zero, in which case the non-zero value will be used. If both values are zero, no limit is enforced.
    enforcedTargetLimit: false


    ## Per-scrape limit on number of labels that will be accepted for a sample. If more than this number of labels are present
    ## post metric-relabeling, the entire scrape will be treated as failed. 0 means no limit. Only valid in Prometheus versions
    ## 2.27.0 and newer.
    enforcedLabelLimit: false

    ## Per-scrape limit on length of labels name that will be accepted for a sample. If a label name is longer than this number
    ## post metric-relabeling, the entire scrape will be treated as failed. 0 means no limit. Only valid in Prometheus versions
    ## 2.27.0 and newer.
    enforcedLabelNameLengthLimit: false

    ## Per-scrape limit on length of labels value that will be accepted for a sample. If a label value is longer than this
    ## number post metric-relabeling, the entire scrape will be treated as failed. 0 means no limit. Only valid in Prometheus
    ## versions 2.27.0 and newer.
    enforcedLabelValueLengthLimit: false

    ## AllowOverlappingBlocks enables vertical compaction and vertical query merge in Prometheus. This is still experimental
    ## in Prometheus so it may change in any upcoming release.
    allowOverlappingBlocks: false

    ## Minimum number of seconds for which a newly created pod should be ready without any of its container crashing for it to
    ## be considered available. Defaults to 0 (pod will be considered available as soon as it is ready).
    minReadySeconds: 0

    # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico),
    # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working
    # Use the host's network namespace if true. Make sure to understand the security implications if you want to enable it.
    # When hostNetwork is enabled, this will set dnsPolicy to ClusterFirstWithHostNet automatically.
    hostNetwork: false

    # HostAlias holds the mapping between IP and hostnames that will be injected
    # as an entry in the pods hosts file.
    hostAliases: []
    #  - ip: 10.10.0.100
    #    hostnames:
    #      - a1.app.local
    #      - b1.app.local

    ## TracingConfig configures tracing in Prometheus.
    ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#prometheustracingconfig
    tracingConfig: {}

    ## Additional configuration which is not covered by the properties above. (passed through tpl)
    additionalConfig: {}

    ## Additional configuration which is not covered by the properties above.
    ## Useful, if you need advanced templating inside alertmanagerSpec.
    ## Otherwise, use prometheus.prometheusSpec.additionalConfig (passed through tpl)
    additionalConfigString: ""

    ## Defines the maximum time that the `prometheus` container's startup probe
    ## will wait before being considered failed. The startup probe will return
    ## success after the WAL replay is complete. If set, the value should be
    ## greater than 60 (seconds). Otherwise it will be equal to 900 seconds (15
    ## minutes).
    maximumStartupDurationSeconds: 0

  additionalRulesForClusterRole: []
  #  - apiGroups: [ "" ]
  #    resources:
  #      - nodes/proxy
  #    verbs: [ "get", "list", "watch" ]

  additionalServiceMonitors: []
  # - name: "emojivoto-monitor"
  #   selector:
  #     matchExpressions:
  #       - key: app
  #         operator: In
  #         values:
  #           - emoji-svc
  #           - voting-svc
  #   namespaceSelector:
  #     matchNames:
  #       - emojivoto
  #   endpoints:
  #     - port: "prom"

    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    ##
    # metricRelabelings: []
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]

    ## RelabelConfigs to apply to samples before scraping
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    ##
    # relabelings: []
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace

  additionalPodMonitors: []
  ## Name of the PodMonitor to create
  ##
  # - name: ""

    ## Additional labels to set used for the PodMonitorSelector. Together with standard labels from
    ## the chart
    ##
    # additionalLabels: {}

    ## Pod label for use in assembling a job name of the form <label value>-<port>
    ## If no label is specified, the pod endpoint name is used.
    ##
    # jobLabel: ""

    ## Label selector for pods to which this PodMonitor applies
    ##
    # selector: {}

    ## PodTargetLabels transfers labels on the Kubernetes Pod onto the target.
    ##
    # podTargetLabels: {}

    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
    ##
    # sampleLimit: 0

    ## Namespaces from which pods are selected
    ##
    # namespaceSelector:
      ## Match any namespace
      ##
      # any: false

      ## Explicit list of namespace names to select
      ##
      # matchNames: []

    ## Endpoints of the selected pods to be monitored
    ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#podmetricsendpoint
    ##
    # podMetricsEndpoints: []

## Configuration for thanosRuler
## ref: https://thanos.io/tip/components/rule.md/
##
thanosRuler:

  ## Deploy thanosRuler
  ##
  enabled: false

  ## Annotations for ThanosRuler
  ##
  annotations: {}

  ## Service account for ThanosRuler to use.
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  ##
  serviceAccount:
    create: true
    name: ""
    annotations: {}

  ## Configure pod disruption budgets for ThanosRuler
  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget
  ##
  podDisruptionBudget:
    enabled: false
    minAvailable: 1
    maxUnavailable: ""

  ingress:
    enabled: false

    # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
    # ingressClassName: nginx

    annotations: {}

    labels: {}

    ## Hosts must be provided if Ingress is enabled.
    ##
    hosts: []
      # - thanosruler.domain.com

    ## Paths to use for ingress rules - one path should match the thanosruler.routePrefix
    ##
    paths: []
    # - /

    ## For Kubernetes >= 1.18 you should specify the pathType (determines how Ingress paths should be matched)
    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
    # pathType: ImplementationSpecific

    ## TLS configuration for ThanosRuler Ingress
    ## Secret must be manually created in the namespace
    ##
    tls: []
    # - secretName: thanosruler-general-tls
    #   hosts:
    #   - thanosruler.example.com

  ## Configuration for ThanosRuler service
  ##
  service:
    annotations: {}
    labels: {}
    clusterIP: ""
    ipDualStack:
      enabled: false
      ipFamilies: ["IPv6", "IPv4"]
      ipFamilyPolicy: "PreferDualStack"

    ## Port for ThanosRuler Service to listen on
    ##
    port: 10902
    ## To be used with a proxy extraContainer port
    ##
    targetPort: 10902
    ## Port to expose on each node
    ## Only used if service.type is 'NodePort'
    ##
    nodePort: 30905
    ## List of IP addresses at which the Prometheus server service is available
    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
    ##

    ## Additional ports to open for ThanosRuler service
    additionalPorts: []
    externalIPs: []
    loadBalancerIP: ""
    loadBalancerSourceRanges: []

    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
    externalTrafficPolicy: Cluster

    ## Service type
    type: ClusterIP

  ## Configuration for creating a ServiceMonitor for the ThanosRuler service
  serviceMonitor:
    selfMonitor: true
    interval: ""
    additionalLabels:
      prometheus.io/scrap-with: kube-prometheus-stack

    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
    sampleLimit: 0

    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
    targetLimit: 0

    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    labelLimit: 0

    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    labelNameLengthLimit: 0

    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    labelValueLengthLimit: 0

    ## proxyUrl: URL of a proxy that should be used for scraping.
    proxyUrl: ""

    ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.
    scheme: ""

    ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.
    ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api.md#tlsconfig
    tlsConfig: {}
    bearerTokenFile:

    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    metricRelabelings: []
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]

    ## RelabelConfigs to apply to samples before scraping
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    relabelings: []
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace

    ## Additional Endpoints
    ##
    additionalEndpoints: []
    # - port: oauth-metrics
    #   path: /metrics

  ## Settings affecting thanosRulerpec
  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#thanosrulerspec
  ##
  thanosRulerSpec:
    podMetadata: {}
    ruleNamespaceSelector: {}
    ruleSelectorNilUsesHelmValues: false
    ruleSelector: {}
    logFormat: logfmt
    logLevel: info
    replicas: 1
    retention: 24h
    evaluationInterval: ""
    storage: {}
    alertmanagersConfig:
      existingSecret: {}
      secret: {}
    externalPrefix:
    externalPrefixNilUsesHelmValues: true
    routePrefix: /
    objectStorageConfig:
      # use existing secret, if configured, objectStorageConfig.secret will not be used
      existingSecret: {}
        # name: ""
        # key: ""
      # will render objectStorageConfig secret data and configure it to be used by Thanos Ruler custom resource, ignored when objectStorageConfig.existingSecret is set
      # https://thanos.io/tip/thanos/storage.md/#s3
      secret: {}
        # type: S3
        # config:
        #   bucket: ""
        #   endpoint: ""
        #   region: ""
        #   access_key: ""
        #   secret_key: ""

    ## Labels by name to drop before sending to alertmanager
    ## Maps to the --alert.label-drop flag of thanos ruler.
    alertDropLabels: []

    ## QueryEndpoints defines Thanos querier endpoints from which to query metrics.
    ## Maps to the --query flag of thanos ruler.
    queryEndpoints: []

    ## Define configuration for connecting to thanos query instances. If this is defined, the queryEndpoints field will be ignored.
    ## Maps to the query.config CLI argument. Only available with thanos v0.11.0 and higher.
    queryConfig:
      # use existing secret, if configured, queryConfig.secret will not be used
      existingSecret: {}
        # name: ""
        # key: ""
      # render queryConfig secret data and configure it to be used by Thanos Ruler custom resource, ignored when queryConfig.existingSecret is set
      # https://thanos.io/tip/components/rule.md/#query-api
      secret: {}
        # - http_config:
        #     basic_auth:
        #       username: some_user
        #       password: some_pass
        #   static_configs:
        #     - URL
        #   scheme: http
        #   timeout: 10s

    ## Labels configure the external label pairs to ThanosRuler. A default replica
    ## label `thanos_ruler_replica` will be always added as a label with the value
    ## of the pod's name and it will be dropped in the alerts.
    labels: {}

    ## If set to true all actions on the underlying managed objects are not going to be performed, except for delete actions.
    ##
    paused: false

    ## Allows setting additional arguments for the ThanosRuler container
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#thanosruler
    ##
    additionalArgs: []
      # - name: remote-write.config
      #   value: |-
      #     "remote_write":
      #     - "name": "receiver-0"
      #       "remote_timeout": "30s"
      #       "url": "http://thanos-receiver-0.thanos-receiver:8081/api/v1/receive"

    ## Define which Nodes the Pods are scheduled on.
    ## ref: https://kubernetes.io/docs/user-guide/node-selection/
    ##
    nodeSelector: {}

    ## Define resources requests and limits for single Pods.
    ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
    ##
    resources: {}
    # requests:
    #   memory: 400Mi

    ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.
    ## The default value "soft" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.
    ## The value "hard" means that the scheduler is *required* to not schedule two replica pods onto the same node.
    ## The value "" will disable pod anti-affinity so that no anti-affinity rules will be configured.
    ##
    podAntiAffinity: ""

    ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.
    ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone
    ##
    podAntiAffinityTopologyKey: kubernetes.io/hostname

    ## Assign custom affinity rules to the thanosRuler instance
    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
    ##
    affinity: {}
    # nodeAffinity:
    #   requiredDuringSchedulingIgnoredDuringExecution:
    #     nodeSelectorTerms:
    #     - matchExpressions:
    #       - key: kubernetes.io/e2e-az-name
    #         operator: In
    #         values:
    #         - e2e-az1
    #         - e2e-az2

    ## If specified, the pod's tolerations.
    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    ##
    tolerations: []
    # - key: "key"
    #   operator: "Equal"
    #   value: "value"
    #   effect: "NoSchedule"

    ## If specified, the pod's topology spread constraints.
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/
    ##
    topologySpreadConstraints: []
    # - maxSkew: 1
    #   topologyKey: topology.kubernetes.io/zone
    #   whenUnsatisfiable: DoNotSchedule
    #   labelSelector:
    #     matchLabels:
    #       app: thanos-ruler

    ## SecurityContext holds pod-level security attributes and common container settings.
    ## This defaults to non root user with uid 1000 and gid 2000. *v1.PodSecurityContext  false
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
    ##
    securityContext:
      runAsGroup: 2000
      runAsNonRoot: true
      runAsUser: 1000
      fsGroup: 2000
      seccompProfile:
        type: RuntimeDefault

    ## ListenLocal makes the ThanosRuler server listen on loopback, so that it does not bind against the Pod IP.
    ## Note this is only for the ThanosRuler UI, not the gossip communication.
    ##
    listenLocal: false

    ## Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to an ThanosRuler pod.
    ##
    containers: []

    # Additional volumes on the output StatefulSet definition.
    volumes: []

    # Additional VolumeMounts on the output StatefulSet definition.
    volumeMounts: []

    ## InitContainers allows injecting additional initContainers. This is meant to allow doing some changes
    ## (permissions, dir tree) on mounted volumes before starting prometheus
    initContainers: []

    ## Priority class assigned to the Pods
    ##
    priorityClassName: ""

    ## PortName to use for ThanosRuler.
    ##
    portName: "web"

    ## Additional configuration which is not covered by the properties above. (passed through tpl)
    additionalConfig: {}

    ## Additional configuration which is not covered by the properties above.
    ## Useful, if you need advanced templating
    additionalConfigString: ""

  ## ExtraSecret can be used to store various data in an extra secret
  ## (use it for example to store hashed basic auth credentials)
  extraSecret:
    ## if not set, name will be auto generated
    # name: ""
    annotations: {}
    data: {}
  #   auth: |
  #     foo:$apr1$OFG3Xybp$ckL0FHDAkoXYIlH9.cysT0
  #     someoneelse:$apr1$DMZX2Z4q$6SbQIfyuLQd.xmo/P0m2c.

## Setting to true produces cleaner resource names, but requires a data migration because the name of the persistent volume changes. Therefore this should only be set once on initial installation.
##
cleanPrometheusOperatorObjectNames: false

## Extra manifests to deploy as an array
extraManifests: []
  # - apiVersion: v1
  #   kind: ConfigMap
  #   metadata:
  #   labels:
  #     name: prometheus-extra
  #   data:
  #     extra-data: "value"
